# -*- coding: utf-8 -*-
"""ZeroFed Sector-wise ST-global TCN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ItPSATXlN7hqe6cjJ03RiBrHQUOzi1qw
"""

import warnings
warnings.filterwarnings("ignore")

import torch.nn.functional as F
from sklearn.metrics import precision_recall_fscore_support, f1_score
import sklearn.metrics as metrics_sk
from sklearn.metrics import classification_report
from collections import OrderedDict
from typing import List, Tuple
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from sklearn.model_selection import train_test_split
from datetime import datetime, timedelta
import itertools as it
from torch.utils.data import TensorDataset, DataLoader, Dataset

from sklearn import metrics as metrics_sk
from sklearn.preprocessing import MinMaxScaler

DEVICE = torch.device("cpu") # Try "cuda" to train on GPU

BATCH_SIZE = 32
time_frequency = 60 * 24
chunk_size = 10

def datetime_range(start, end, delta):
    current = start
    while current < end:
        yield current
        current += delta

def moving_window(x, length, step=1):
    streams = it.tee(x, length)
    return zip(*[it.islice(stream, i, None, step) for stream, i in zip(streams, it.count(step=step))])

def choose_target_generate_fllist(sheroaks_crime):
    crime_type = 8
    neighborhood_type = 113
    start_time_so = '2018-01-01'
    end_time_so = '2018-12-31'
    format_string = '%Y-%m-%d'
    start_time_so = datetime.strptime(start_time_so, format_string)
    end_time_so = datetime.strptime(end_time_so, format_string)
    time_list_so = [dt.strftime('%Y-%m-%d') for dt in datetime_range(start_time_so, end_time_so, timedelta(minutes=time_frequency))]
    x_ = list(moving_window(time_list_so, chunk_size))
    final_list_so = []
    label_list_so = []
    for i in range(0, len(x_)):
            feature_time_frame = x_[i][:chunk_size-1]
            feature_list = []
            ##fix a bug here
            for index_fea in range(0, len(feature_time_frame) - 1):
                start_so = feature_time_frame[index_fea]
                end_so = feature_time_frame[index_fea + 1]
                df_so_middle = sheroaks_crime.loc[(sheroaks_crime['date_occ'] >= start_so) & (sheroaks_crime['date_occ'] < end_so)]
                crime_record = np.zeros((neighborhood_type, crime_type))
                for index, row in df_so_middle.iterrows():
                    crime_record[int(row["neighborhood_id"])][int(row["crime_type_id"])] = 1
                feature_list.append(crime_record)
            final_list_so.append(feature_list)

            label_time_frame = x_[i][chunk_size-2:]
            label_time_slots = sheroaks_crime.loc[(sheroaks_crime['date_occ'] >= label_time_frame[0]) & (sheroaks_crime['date_occ'] < label_time_frame[1])]
            crime_record = np.zeros((neighborhood_type, crime_type))
            for index_label, row_label in label_time_slots.iterrows():
                crime_record[int(row_label["neighborhood_id"])][int(row_label["crime_type_id"])] = 1
            label_list_so.append(crime_record)


    print("the shape of feature list is {}, and the shape of label list is {} ".format(np.shape(final_list_so), np.shape(label_list_so)))
    return final_list_so, label_list_so


# def choose_target_generate_fllist(sheroaks_crime):
#     crime_type = 8
#     neighborhood_type = 77
#     start_time_so = '1/1/2015'
#     end_time_so = '12/31/2015'
#     format_string = '%m/%d/%Y'
#     start_time_so = datetime.strptime(start_time_so, format_string)
#     end_time_so = datetime.strptime(end_time_so, format_string)
#     time_list_so = [dt.strftime('%m/%d/%Y') for dt in
#                     datetime_range(start_time_so, end_time_so, timedelta(minutes=time_frequency))]

#     x_ = list(moving_window(time_list_so, chunk_size))

#     # Ensure date column in DataFrame is in datetime format
#     sheroaks_crime['date'] = pd.to_datetime(sheroaks_crime['date'], format='%m/%d/%Y', errors='coerce')

#     final_list_so = []
#     label_list_so = []
#     for i in range(0, len(x_)):
#             feature_time_frame = x_[i][:chunk_size-1]
#             feature_list = []
#             ##fix a bug here
#             for index_fea in range(0, len(feature_time_frame) - 1):
#                 start_so = feature_time_frame[index_fea]
#                 end_so = feature_time_frame[index_fea + 1]
#                 df_so_middle = sheroaks_crime.loc[(sheroaks_crime['date'] >= start_so) & (sheroaks_crime['date'] < end_so)]
#                 crime_record = np.zeros((neighborhood_type, crime_type))
#                 for index, row in df_so_middle.iterrows():
#                     crime_record[int(row["neighborhood_id"])][int(row["crime_type_id"])] = 1
#                 feature_list.append(crime_record)
#             final_list_so.append(feature_list)

#             label_time_frame = x_[i][chunk_size-2:]
#             label_time_slots = sheroaks_crime.loc[(sheroaks_crime['date'] >= label_time_frame[0]) & (sheroaks_crime['date'] < label_time_frame[1])]
#             crime_record = np.zeros((neighborhood_type, crime_type))
#             for index_label, row_label in label_time_slots.iterrows():
#                 crime_record[int(row_label["neighborhood_id"])][int(row_label["crime_type_id"])] = 1
#             label_list_so.append(crime_record)


#     print("the shape of feature list is {}, and the shape of label list is {} ".format(np.shape(final_list_so), np.shape(label_list_so)))
#     return final_list_so, label_list_so

def plot_crime():
    df = pd.read_csv('updated_crime_CHI.csv')
    grouped_counts = df.groupby('neighborhood_id').size()
    print(grouped_counts)
    grouped_counts_sorted = grouped_counts.sort_values(ascending=False)
    # Plot size of each group
    plt.figure(figsize=(18, 6))
    grouped_counts_sorted.plot(kind='bar')
    # Add labels and legend
    plt.xlabel('# Neighborhoods')
    plt.ylabel('# Crimes')
    # plt.title('Plot of DataFrame grouped by Neighborhood')
    # Remove x-axis tick labels
    #plt.xticks([])
    # Save plot as PDF
    #plt.grid(True)
    #plt.savefig('plot-neighborhood.pdf', bbox_inches='tight', format='pdf')
    plt.show()

plot_crime()

def plot_crime_type(df):
    #df = pd.read_csv('processed_crime.csv')
    grouped_counts = df.groupby('crime_type_id').size()
    print(grouped_counts)
    grouped_counts_sorted = grouped_counts.sort_values(ascending=False)
    # Plot size of each group
    plt.figure(figsize=(8, 4))
    grouped_counts_sorted.plot(kind='bar')
    # Add labels and legend
    plt.xlabel('Crime Types')
    plt.ylabel('# Crimes')
    # plt.title('Plot of DataFrame grouped by Neighborhood')
    # Remove x-axis tick labels
    #plt.xticks([])
    # Save plot as PDF
    #plt.grid(True)
    #plt.savefig('plot-crimetype.pdf', bbox_inches='tight', format='pdf')
    plt.show()
#plot_crime_type()

class CustomDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        feature = torch.tensor(self.features[idx], dtype=torch.float32)
        label = torch.tensor(self.labels[idx], dtype=torch.float32)
        return feature, label

def load_datasets():

    #PATH = 'C:\\Users\\cbhum\\OneDrive\\Documents\\Datasets\\crime\\'
    df = pd.read_csv('processed_crime.csv')

    # Create train/val for each partition and wrap it into DataLoader
    trainloaders = []
    valloaders = []

    feature, label = choose_target_generate_fllist(df)
    num_samples = len(feature)

    # Select 10 sectors from the original feature list
    selected_sector_indices = [25, 49, 18, 29, 81, 102, 80, 87, 92, 110]
    # Extract the selected sectors from the original feature list

    # print(type(feature))

    # print(type(selected_sector_indices))
    feature_np = np.array(feature)
    label_np = np.array(label)

    print(feature_np.shape)
    print(label_np.shape)

    selected_feature = feature_np[:, :, selected_sector_indices, :]

    # Extract the selected sectors from the original label list
    selected_label = label_np[:, selected_sector_indices, :]

    print(selected_feature.shape)
    print(selected_label.shape)

    # Calculate the sizes for training, validation, test, and global test sets
    num_train = round(num_samples *  0.5417)  # 65% for training
    num_val = round(num_samples * 0.0417)  # 5% for validation
    num_test = round(num_samples * 0.0833)  # 10% for testing

    # Training set
    x_train, target_train = selected_feature[:num_train], selected_label[:num_train]

    # Validation set
    x_val, target_val = selected_feature[num_train:num_train + num_val], selected_label[num_train:num_train + num_val]

    # Test set
    #x_test, target_test = selected_feature[num_train + num_val + (3 * num_test): num_train + num_val + (4 * num_test)], selected_label[num_train + num_val + (3 * num_test): num_train + num_val + (4 * num_test)]
    x_test, target_test = selected_feature[num_train + num_val :  num_train + num_val + num_test], selected_label[num_train + num_val : num_train + num_val + num_test ]
    train_dataset = CustomDataset(x_train, target_train)
    val_dataset = CustomDataset(x_val, target_val)
    test_dataset = CustomDataset(x_test, target_test)

    trainloaders = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)
    valloaders = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
    testloaders = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    return trainloaders, valloaders, testloaders

trainloaders, valloaders, testloaders = load_datasets()

# class Net(nn.Module):
#     def __init__(self, num_inputs = 10*8, hidden_units = 16):
#         super().__init__()
#         self.num_inputs = num_inputs  # this is the number of features
#         self.hidden_units = hidden_units
#         self.num_layers = 1
#         #print("client model Initialization starting")

#         self.lstm = nn.LSTM(
#             input_size=num_inputs,
#             hidden_size=hidden_units,
#             batch_first=True,
#             num_layers=self.num_layers
#         )

#         self.linear = nn.Linear(in_features=self.hidden_units, out_features=num_inputs)
#         self.sigmoid = nn.Sigmoid()
#         self.relu = nn.ReLU()
#         #print("client model Initialization Complete")

#     def forward(self, x):
#         #print("I am in the starting of forward pass of client")
#         batch_size = x.shape[0]
#         h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units)
#         c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units)

#         _, (hn, _) = self.lstm(x, (h0, c0))

#         out = self.relu(hn[0])
#         out = self.linear(out)
#         out = self.sigmoid(out)

#         return out

class TCNBlock(nn.Module):
    """ A single TCN residual block """

    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1):
        super(TCNBlock, self).__init__()
        padding = (kernel_size - 1) * dilation // 2  # Keep output size same
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation)
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=padding, dilation=dilation)
        self.relu = nn.ReLU()
        self.residual = nn.Conv1d(in_channels, out_channels,
                                  kernel_size=1) if in_channels != out_channels else nn.Identity()

    def forward(self, x):
        res = self.residual(x)
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        return x + res  # Residual connection


class Net(nn.Module):
    def __init__(self, num_inputs=10*8, hidden_units=16, num_layers=3, kernel_size=3):
        super().__init__()
        self.num_inputs = num_inputs
        self.hidden_units = hidden_units
        # TCN Layers
        layers = []
        for i in range(num_layers):
            in_channels = num_inputs if i == 0 else hidden_units
            layers.append(TCNBlock(in_channels, hidden_units, kernel_size, dilation=2 ** i))  # Increasing dilation
        self.tcn = nn.Sequential(*layers)
        # Fully connected layers for outputs
        self.fc_zero = nn.Linear(hidden_units, 10*8)  # Zero-inflation probability
        self.fc_nb_mean = nn.Linear(hidden_units, 10*8)  # Negative binomial mean
        self.fc_nb_disp = nn.Linear(hidden_units, 10*8)
        # Activation functions
        self.sigmoid = nn.Sigmoid()
        self.softplus = nn.Softplus()

    def forward(self, x):
        x = x.permute(0, 2, 1)  # Change to (batch, channels, sequence_length) for TCN
        tcn_out = self.tcn(x)
        out = tcn_out[:, :, -1]  # Take last time step output
        zero_prob = self.sigmoid(self.fc_zero(out))  # Zero-inflation probability
        nb_mean = torch.exp(self.fc_nb_mean(out))  # NB mean (ensured positive)
        nb_disp = self.softplus(self.fc_nb_disp(out))  # NB dispersion (positive via softplus)

        return zero_prob, nb_mean, nb_disp

def zimnb_loss(zero_prob, nb_mean, nb_disp, targets):
    eps = 1e-10  # Small value to prevent log(0)

    # Binary cross-entropy for zero-inflation part
    zero_loss = targets.eq(0).float() * torch.log(zero_prob + eps)

    # Negative binomial log likelihood for non-zero counts
    targets_nonzero = targets.gt(0).float()
    log_factorial = torch.lgamma(targets + 1)

    # NB likelihood (log NB PMF)
    nb_loss = targets_nonzero * (
            torch.lgamma(targets + nb_disp) - torch.lgamma(nb_disp)
            - log_factorial + nb_disp * (torch.log(nb_disp) - torch.log(nb_disp + nb_mean))
            + targets * (torch.log(nb_mean) - torch.log(nb_disp + nb_mean))
    )

    # Total loss (negative log likelihood)
    total_loss = -torch.mean(zero_loss + nb_loss)

    return total_loss

class DynamicWeightedBCELoss(nn.Module):
    def __init__(self, num_classes):
        super(DynamicWeightedBCELoss, self).__init__()
        self.weights = nn.Parameter(torch.ones(num_classes))

    def forward(self, outputs, targets):
        # Apply sigmoid activation to outputs
        #outputs = torch.sigmoid(outputs)

        # Compute binary cross-entropy loss
        bce_loss = F.binary_cross_entropy(outputs, targets, reduction='none')

        # Apply class weights
        weighted_bce_loss = bce_loss * self.weights

        # Average the loss over all samples
        loss = torch.mean(weighted_bce_loss)

        return loss

class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, outputs, targets):
        bce_loss = F.binary_cross_entropy_with_logits(outputs, targets, reduction='none')
        pt = torch.exp(-bce_loss)
        focal_loss = self.alpha * (1 - pt) ** self.gamma * bce_loss

        if self.reduction == 'mean':
            return torch.mean(focal_loss)
        elif self.reduction == 'sum':
            return torch.sum(focal_loss)
        else:
            return focal_loss

class DiceLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super(DiceLoss, self).__init__()
        self.smooth = smooth

    def forward(self, outputs, targets):
        # Apply sigmoid activation to outputs
        outputs = torch.sigmoid(outputs)

        # Flatten tensors to 2D (batch_size x num_classes)
        outputs = outputs.view(outputs.size(0), -1)
        targets = targets.view(targets.size(0), -1)

        # Calculate intersection and union
        intersection = torch.sum(outputs * targets, dim=1) + self.smooth
        union = torch.sum(outputs, dim=1) + torch.sum(targets, dim=1) + self.smooth

        # Calculate Dice coefficient
        dice_coefficient = (2. * intersection) / union

        # Average the Dice coefficient over all samples
        dice_loss = 1.0 - dice_coefficient.mean()

        return dice_loss

class JaccardLoss(nn.Module):
    def __init__(self, smooth=1e-6):
        super(JaccardLoss, self).__init__()
        self.smooth = smooth

    def forward(self, outputs, targets):
        # Apply sigmoid activation to outputs
        outputs = torch.sigmoid(outputs)

        # Calculate intersection and union
        intersection = torch.sum(outputs * targets, dim=1)
        union = torch.sum(outputs, dim=1) + torch.sum(targets, dim=1) - intersection + self.smooth

        # Calculate Jaccard similarity coefficient
        jaccard_coefficient = (intersection + self.smooth) / union

        # Average the Jaccard coefficient over all samples
        jaccard_loss = 1.0 - jaccard_coefficient.mean()

        return jaccard_loss

class FocalTverskyLoss(nn.Module):
    def __init__(self, alpha=0.5, beta=0.5, gamma=1, smooth=1e-6):
        super(FocalTverskyLoss, self).__init__()
        self.alpha = alpha
        self.beta = beta
        self.gamma = gamma
        self.smooth = smooth

    def forward(self, outputs, targets):
        # Apply sigmoid activation to outputs
        #outputs = torch.sigmoid(outputs)

        # Flatten tensors to 2D (batch_size x num_classes)
        outputs = outputs.view(outputs.size(0), -1)
        targets = targets.view(targets.size(0), -1)

        # Calculate Tversky numerator and denominator
        true_positives = torch.sum(outputs * targets, dim=1)
        false_positives = torch.sum(outputs * (1 - targets), dim=1)
        false_negatives = torch.sum((1 - outputs) * targets, dim=1)
        tversky_num = true_positives + self.smooth
        tversky_denom = true_positives + self.alpha * false_positives + self.beta * false_negatives + self.smooth

        # Calculate Tversky loss
        tversky_loss = 1.0 - (tversky_num / tversky_denom)

        # Apply focal loss
        focal_loss = torch.pow(tversky_loss, self.gamma)

        # Average the focal loss over all samples
        focal_tversky_loss = focal_loss.mean()

        return focal_tversky_loss

def sigmoid(array):
    for i in range(len(array)):
        for j in range(len(array[i])):
            array[i][j] = 1/(1 + np.exp(-array[i][j]))
    return array

def train(net, trainloader, epochs: int, verbose=False):
    """Train the network on the training set."""
    all_predictions, all_labels, macro, micro = [], [], [], []
    learning_rate = 0.001
    criterion = torch.nn.BCELoss()
    #criterion = DiceLoss(smooth=1e-6)
    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)
    net.train()
    loss_history = []  # List to store loss values after each epoch
    size = len(trainloader.dataset)
    correct_train, total_train, epoch_loss_train = 0, 0, 0.0

    for batch in trainloader:
        x, labels = batch[0], batch[1]
        optimizer.zero_grad()
        x_shape = x.shape
        x = x.reshape(x_shape[0], x_shape[1], -1)
        labels_shape = labels.shape
        labels = labels.reshape(labels_shape[0], -1)
        zero_prob, mu, dispersion  = net(x)
        #print(outputs.shape)
        #print(labels.shape)
        loss = zimnb_loss(zero_prob, mu, dispersion, labels)
        loss.backward()
        optimizer.step()
        # Metrics
        epoch_loss_train += loss.item()
        total_train += labels.size(0)
        predicted_labels = (zero_prob > 0.5).float()
        all_predictions.extend(predicted_labels.tolist())
        all_labels.extend(labels.tolist())
        # Compute accuracy for each sample and each label
        correct_train += ((predicted_labels == labels).float().sum(dim=1) == labels.size(1)).sum().item()

    loss_history.append(epoch_loss_train)
    epoch_acc = correct_train / total_train
    epoch_loss_train /= len(trainloader)
    all_labels_reshape = np.reshape(all_labels, (-1, 8))
    all_predictions_reshape = np.reshape(all_predictions, (-1, 8))
    macro_f1 = metrics_sk.f1_score(all_labels_reshape, all_predictions_reshape, average='macro')
    micro_f1 = metrics_sk.f1_score(all_labels_reshape, all_predictions_reshape, average='micro')

    print(f"Epoch {epoch+1}: train loss {epoch_loss_train}, accuracy {epoch_acc}, macro F1 {macro_f1}, micro F1 {micro_f1}")
    macro.append(macro_f1)
    micro.append(micro_f1)

    return loss_history, epoch_acc, macro, micro



def val(net, valloader):
    """Evaluate the network on the entire test set."""
    all_predictions, all_labels, macro, micro = [], [], [], []
    loss_history, epoch_loss = [], []
    #class_weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])
    output_dim = 8
    #criterion = DynamicWeightedBCELoss(num_classes=output_dim)
    #criterion = FocalLoss(alpha=0.25, gamma=2, reduction='mean')
    #criterion = DiceLoss(smooth=1e-6)
    #criterion = JaccardLoss(smooth=1e-6)
    #criterion = FocalTverskyLoss(alpha=0.5, beta=0.5, gamma=1, smooth=1e-6)
    criterion = torch.nn.BCELoss()
    correct, total, loss = 0, 0, 0.0
    net.eval()
    with torch.no_grad():
        for batch in valloader:
            features, labels = batch[0], batch[1]
            features_shape = features.shape
            features = features.reshape(features_shape[0], features_shape[1], -1)
            labels_shape = labels.shape
            labels = labels.reshape(labels_shape[0],-1)
            zero_prob, mu, dispersion = net(features)
            loss += zimnb_loss(zero_prob, mu, dispersion, labels).item()
            #_, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            #epoch_loss += loss
            predicted_labels = (zero_prob > 0.5).float()
            all_predictions.extend(predicted_labels.tolist())
            all_labels.extend(labels.tolist())
            # Compute accuracy for each sample and each label
            correct += ((predicted_labels == labels).float().sum(dim=1) == labels.size(1)).sum().item()
    loss /= len(valloader.dataset)
    loss_history.append(loss)
    accuracy = correct / total
    all_labels_reshape = np.reshape(all_labels, (-1,8))
    all_predictions_reshape = np.reshape (all_predictions, (-1,8))
    # y_pred_reshape_sigmoid = sigmoid(all_predictions_reshape)
    # ss = MinMaxScaler(feature_range=(0, 1))
    # y_pred_reshape_sigmoid = ss.fit_transform(y_pred_reshape_sigmoid)
    # threshold = np.quantile(y_pred_reshape_sigmoid, 1 - np.mean(all_labels_reshape[1]))
    # y_pred_reshape_sigmoid[y_pred_reshape_sigmoid >= threshold] = 1
    # y_pred_reshape_sigmoid[y_pred_reshape_sigmoid < threshold] = 0
    macro_f1 = metrics_sk.f1_score(all_labels_reshape, all_predictions_reshape, average = 'macro')
    micro_f1 = metrics_sk.f1_score(all_labels_reshape, all_predictions_reshape, average = 'micro')
    # macro_f1 = f1_score(all_labels, all_predictions, average='macro')
    # micro_f1 = f1_score(all_labels, all_predictions, average='micro')
    print(f"Epoch {epoch+1}:  loss {loss}, accuracy {accuracy}, macro F1 {macro_f1}, micro F1 {micro_f1}")

    # precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='macro')
    # macro_f1 = f1_score(all_labels, all_predictions, average='macro')
    # micro_f1 = f1_score(all_labels, all_predictions, average='micro')
    # print(f"Epoch {epoch+1}: Val loss {loss}, accuracy {accuracy}, macro F1 {macro_f1}, micro F1 {micro_f1}")
    macro.append(macro_f1)
    micro.append(micro_f1)
    return loss_history, accuracy, macro, micro

def test(net, testloader):
    """Evaluate the network on the entire test set."""
    all_predictions, all_labels, macro, micro = [], [], [], []
    #class_weights = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0])
    criterion = torch.nn.BCELoss()
    output_dim = 8
    #criterion = DynamicWeightedBCELoss(num_classes=output_dim)
    #criterion = FocalLoss(alpha=0.25, gamma=2, reduction='mean')
    #criterion = DiceLoss(smooth=1e-6)
    #criterion = JaccardLoss(smooth=1e-6)
    #criterion = FocalTverskyLoss(alpha=0.5, beta=0.5, gamma=1, smooth=1e-6)
    correct, total, loss = 0, 0, 0.0
    net.eval()
    with torch.no_grad():
        for batch in testloader:
            features, labels = batch[0], batch[1]
            features_shape = features.shape
            #print("features shape",features.shape)
            features = features.reshape(features_shape[0], features_shape[1], -1)
            labels_shape = labels.shape
            #print(labels.shape)
            labels = labels.reshape(labels_shape[0], -1)
            zero_prob, mu, dispersion = net(features)
            loss += zimnb_loss(zero_prob, mu, dispersion, labels).item()
            #print("Output shape", outputs.shape)
            total += labels.size(0)
            predicted_labels = (zero_prob > 0.5).float()
            all_predictions.extend(predicted_labels.tolist())
            all_labels.extend(labels.tolist())
            # Compute accuracy for each sample and each label
            #correct += ((predicted_labels == labels).float().sum(dim=1) == labels.size(1)).sum().item()
    loss /= len(testloader.dataset)
    #accuracy = correct / total
    all_labels_reshape = np.reshape(all_labels, (-1, 10, 8))
    all_predictions_reshape = np.reshape (all_predictions, (-1,10,8))
    #print("final label shape",all_labels_reshape.shape)
    #print("final prediction shape",all_predictions_reshape.shape)
    # Iterate over each sector (113 sectors)
    #print(type(all_labels_reshape))
    macro_f1 = f1_score(all_labels, all_predictions, average='macro')
    micro_f1 = f1_score(all_labels, all_predictions, average='micro')
    print(f"Test loss {loss}, macro F1 {macro_f1}, micro F1 {micro_f1}")

    for sector_idx in range(all_labels_reshape.shape[1]):
      y_true_sector = all_labels_reshape[:, sector_idx, :]  # True labels for current sector
      y_pred_sector = all_predictions_reshape[:, sector_idx, :]  # Predicted labels for current sector
      # Calculate micro F1 score for current sector
      micro_f1_sector = f1_score(y_true_sector, y_pred_sector, average='micro')
      # Calculate macro F1 score for current sector
      macro_f1_sector = f1_score(y_true_sector, y_pred_sector, average='macro')
      print(f"Sector {sector_idx}: Micro F1 = {micro_f1_sector}, Macro F1 = {macro_f1_sector}")
      macro.append(macro_f1_sector)
      micro.append(micro_f1_sector)


    # total_labels_reshape = np.reshape(all_labels, (-1, 8))
    # total_predictions_reshape = np.reshape (all_predictions, (-1,8))
    # macro_f1 = metrics_sk.f1_score(total_labels_reshape, total_predictions_reshape, average = 'macro')
    # micro_f1 = metrics_sk.f1_score(total_labels_reshape, total_predictions_reshape, average = 'micro')
    #print(f"macro F1 {macro_f1}, micro F1 {micro_f1}")
    #print(".........................Test Metrics........................\n")
    #print(classification_report(all_labels_reshape, all_predictions_reshape))

    return loss, macro, micro

model = Net()
# Training loop
num_epochs = 50
train_losses = []
val_losses = []
epoches = []
val_micros = []
val_macros = []
train_micros = []
train_macros = []
for epoch in range(num_epochs):
    print(f'Epoch no {epoch+1}/{num_epochs}.................\n')
    train_loss, train_acc, train_macro, train_micro = train(model, trainloaders, epoch)
    val_loss, val_acc, test_macro, test_micro = val(model, valloaders)
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_macros.append(train_macro)
    train_micros.append(train_micro)
    val_macros.append(test_macro)
    val_micros.append(test_micro)
    epoches.append(epoch+1)
    #print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')

# Evaluation on test set
test_loss, test_macro, test_micro = test(model, testloaders)


print(f"Test loss {test_loss}, macro F1 {sum(test_macro)/len(test_macro)}, micro F1 {sum(test_micro)/len(test_micro)}")

# Plot loss over epochs
#plt.plot(epoches, train_losses, label='Training Loss')
plt.plot(epoches, val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Over Epochs')
plt.legend()
plt.show()

# Plot both macro and micro F1 scores
plt.plot(epoches, train_macros, label='Macro F1 Score')
plt.plot(epoches, val_macros, label='Micro F1 Score')

# Add labels and title
plt.xlabel('Epochs')
plt.ylabel('F1 Score')
plt.title('Macro and Micro F1 Scores Over Epochs')

# Add legend
plt.legend()

# Show plot
plt.show()