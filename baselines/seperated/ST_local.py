# -*- coding: utf-8 -*-
"""Fedzero ST_Local_TCN_FedCrime.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xFh2TY3BchQjFSlRhzm3CvFnW9n86AYz
"""

import warnings
warnings.filterwarnings("ignore")

import torch.nn.functional as F
from sklearn.metrics import precision_recall_fscore_support, f1_score
import sklearn.metrics as metrics_sk
from sklearn.metrics import classification_report
from collections import OrderedDict
from typing import List, Tuple
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from sklearn.model_selection import train_test_split
from datetime import datetime, timedelta
import itertools as it
from torch.utils.data import TensorDataset, DataLoader, Dataset
from sklearn.metrics import precision_score, recall_score
from sklearn.metrics import average_precision_score

BATCH_SIZE = 32
time_frequency = 60 * 24
chunk_size = 10

from datetime import datetime, timedelta
import numpy as np

def datetime_range(start, end, delta):
    current = start
    while current <= end:
        yield current
        current += delta

def moving_window(x, length, step=1):
    streams = it.tee(x, length)
    return zip(*[it.islice(stream, i, None, step) for stream, i in zip(streams, it.count(step=step))])

def choose_target_generate_fllist(sheroaks_crime):
    crime_type = 8
    start_time_so = '2018-01-01'
    end_time_so = '2018-12-31'
    format_string = '%Y-%m-%d'
    start_time_so = datetime.strptime(start_time_so, format_string)
    end_time_so = datetime.strptime(end_time_so, format_string)
    time_list_so = [dt.strftime('%Y-%m-%d') for dt in datetime_range(start_time_so, end_time_so, timedelta(minutes=time_frequency))]
    x_ = list(moving_window(time_list_so, chunk_size))
    final_list_so = []
    label_list_so = []
    for i in range(len(x_)):
        feature_time_frame = x_[i][:chunk_size-1]
        feature_list = []
        for index_fea in range(len(feature_time_frame) - 1):
            start_so = feature_time_frame[index_fea]
            end_so = feature_time_frame[index_fea + 1]
            df_so_middle = sheroaks_crime.loc[(sheroaks_crime['date_occ'] >= start_so) & (sheroaks_crime['date_occ'] < end_so)]
            crime_record = np.zeros(crime_type)
            for index, row in df_so_middle.iterrows():
                crime_record[int(row["crime_type_id"])] = 1
            feature_list.append(crime_record)
        final_list_so.append(feature_list)

        label_time_frame = x_[i][chunk_size-2:]
        label_time_slots = sheroaks_crime.loc[(sheroaks_crime['date_occ'] >= label_time_frame[0]) & (sheroaks_crime['date_occ'] < label_time_frame[1])]
        crime_record = np.zeros(crime_type)
        for index_label, row_label in label_time_slots.iterrows():
            crime_record[int(row_label["crime_type_id"])] = 1
        label_list_so.append(crime_record)

    print("the shape of feature list is {}, and the shape of label list is {} ".format(np.shape(final_list_so), np.shape(label_list_so)))
    return final_list_so, label_list_so

# def choose_target_generate_fllist(sheroaks_crime):
#     crime_type = 8
#     start_time_so = '1/1/2015'
#     end_time_so = '12/31/2015'
#     format_string = '%m/%d/%Y'
#     start_time_so = datetime.strptime(start_time_so, format_string)
#     end_time_so = datetime.strptime(end_time_so, format_string)
#     time_list_so = [dt.strftime('%m/%d/%Y') for dt in
#                     datetime_range(start_time_so, end_time_so, timedelta(minutes=time_frequency))]

#     x_ = list(moving_window(time_list_so, chunk_size))

#     # Ensure date column in DataFrame is in datetime format
#     sheroaks_crime['date'] = pd.to_datetime(sheroaks_crime['date'], format='%m/%d/%Y', errors='coerce')

#     final_list_so = []
#     label_list_so = []
#     for i in range(len(x_)):
#         feature_time_frame = x_[i][:chunk_size - 1]
#         feature_list = []
#         for index_fea in range(len(feature_time_frame) - 1):
#             start_so = feature_time_frame[index_fea]
#             end_so = feature_time_frame[index_fea + 1]
#             df_so_middle = sheroaks_crime.loc[
#                 (sheroaks_crime['date'] >= start_so) & (sheroaks_crime['date'] < end_so)]
#             crime_record = np.zeros(crime_type)
#             for index, row in df_so_middle.iterrows():
#                 crime_record[int(row["crime_type_id"])] = 1
#             feature_list.append(crime_record)
#         final_list_so.append(feature_list)


#         label_time_frame = x_[i][chunk_size - 2:]
#         label_time_slots = sheroaks_crime.loc[
#             (sheroaks_crime['date'] >= label_time_frame[0]) & (sheroaks_crime['date'] < label_time_frame[1])]
#         crime_record = np.zeros(crime_type)
#         for index_label, row_label in label_time_slots.iterrows():
#             crime_record[int(row_label["crime_type_id"])] = 1
#         label_list_so.append(crime_record)

#     #print("the shape of feature list is {}, and the shape of label list is {} ".format(np.shape(final_list_so), np.shape(label_list_so)))
#     return final_list_so, label_list_so

class CustomDataset(Dataset):
    def __init__(self, features, labels):
        self.features = features
        self.labels = labels

    def __len__(self):
        return len(self.features)

    def __getitem__(self, idx):
        feature = torch.tensor(self.features[idx], dtype=torch.float32)
        label = torch.tensor(self.labels[idx], dtype=torch.float32)
        return feature, label

def load_datasets():

    #PATH = 'C:\\Users\\cbhum\\OneDrive\\Documents\\Datasets\\crime\\'
    df = pd.read_csv('processed_crime.csv')

    # Create train/val for each partition and wrap it into DataLoader
    trainloaders = []
    valloaders = []

    group_partition = df.groupby('neighborhood_id')

    # Iterate over groups and print them
    time_shift = chunk_size

    for partition_id, partition_df in group_partition:
        if partition_id == 110:
           feature, label = choose_target_generate_fllist(partition_df)
           num_samples = len(feature)
           # Calculate the sizes for training, validation, test, and global test sets
           num_train = round(num_samples *  0.5417)  # 65% for training
           num_val = round(num_samples * 0.0417)  # 5% for validation
           num_test = round(num_samples * 0.0833)  # 10% for testing
           num_global_test = num_samples - num_train - num_val - num_test  # Remaining for global testing

           # Training set
           x_train, target_train = feature[:num_train], label[:num_train]

           # Validation set
           x_val, target_val = feature[num_train : num_train + num_val], label[num_train : num_train + num_val]

           # Test set
           x_test, target_test = feature[num_train + num_val  : num_train + num_val + num_test], label[num_train + num_val : num_train + num_val + num_test]

           # Global Test set
           #x_global_test, target_global_test = feature[num_train + num_val : num_train + num_val + num_test], label[num_train + num_val :num_train + num_val +  num_test]


           train_dataset = CustomDataset(x_train, target_train)
           val_dataset = CustomDataset(x_val, target_val)
           test_dataset = CustomDataset(x_test, target_test)

           trainloaders = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)
           valloaders = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
           testloaders = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

           #  for batch_idx, (features, labels) in enumerate(testloaders):
           #       print(f'Batch {batch_idx + 1}:')
           #       print('Features (x) shape:', features.shape)
           #       print('Labels (y) shape:', labels.shape)

    return trainloaders, valloaders, testloaders

trainloaders, valloaders, testloaders = load_datasets()

class TCNBlock(nn.Module):
    """ A single TCN residual block """

    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1):
        super(TCNBlock, self).__init__()
        padding = (kernel_size - 1) * dilation // 2  # Keep output size same
        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=padding, dilation=dilation)
        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=padding, dilation=dilation)
        self.relu = nn.ReLU()
        self.residual = nn.Conv1d(in_channels, out_channels,
                                  kernel_size=1) if in_channels != out_channels else nn.Identity()

    def forward(self, x):
        res = self.residual(x)
        x = self.relu(self.conv1(x))
        x = self.relu(self.conv2(x))
        return x + res  # Residual connection


class Net(nn.Module):
    def __init__(self, num_inputs=8, hidden_units=16, num_layers=3, kernel_size=3):
        super().__init__()
        self.num_inputs = num_inputs
        self.hidden_units = hidden_units
        # TCN Layers
        layers = []
        for i in range(num_layers):
            in_channels = num_inputs if i == 0 else hidden_units
            layers.append(TCNBlock(in_channels, hidden_units, kernel_size, dilation=2 ** i))  # Increasing dilation
        self.tcn = nn.Sequential(*layers)
        # Fully connected layers for outputs
        self.fc_zero = nn.Linear(hidden_units, 8)  # Zero-inflation probability
        self.fc_nb_mean = nn.Linear(hidden_units, 8)  # Negative binomial mean
        self.fc_nb_disp = nn.Linear(hidden_units, 8)
        # Activation functions
        self.sigmoid = nn.Sigmoid()
        self.softplus = nn.Softplus()

    def forward(self, x):
        x = x.permute(0, 2, 1)  # Change to (batch, channels, sequence_length) for TCN
        tcn_out = self.tcn(x)
        out = tcn_out[:, :, -1]  # Take last time step output
        zero_prob = self.sigmoid(self.fc_zero(out))  # Zero-inflation probability
        nb_mean = torch.exp(self.fc_nb_mean(out))  # NB mean (ensured positive)
        nb_disp = self.softplus(self.fc_nb_disp(out))  # NB dispersion (positive via softplus)

        return zero_prob, nb_mean, nb_disp

"""# Different losses"""

def zimnb_loss(zero_prob, nb_mean, nb_disp, targets):
    eps = 1e-10  # Small value to prevent log(0)

    # Binary cross-entropy for zero-inflation part
    zero_loss = targets.eq(0).float() * torch.log(zero_prob + eps)

    # Negative binomial log likelihood for non-zero counts
    targets_nonzero = targets.gt(0).float()
    log_factorial = torch.lgamma(targets + 1)

    # NB likelihood (log NB PMF)
    nb_loss = targets_nonzero * (
            torch.lgamma(targets + nb_disp) - torch.lgamma(nb_disp)
            - log_factorial + nb_disp * (torch.log(nb_disp) - torch.log(nb_disp + nb_mean))
            + targets * (torch.log(nb_mean) - torch.log(nb_disp + nb_mean))
    )

    # Total loss (negative log likelihood)
    total_loss = -torch.mean(zero_loss + nb_loss)

    return total_loss

"""# Train, Val and Test set"""

def train(net, trainloader, epochs: int, verbose=False):
    """Train the network on the training set."""
    all_predictions, all_labels = [], []
    learning_rate = 0.001
    #output_dim = 8 #Uncomment for dynamic weighted BCE loss
    criterion = torch.nn.BCELoss()
    #criterion = DiceLoss(smooth=1e-6)
    #criterion = DiceMacroLoss(smooth=1e-6)
    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)
    net.train()
    loss_history = []  # List to store loss values after each epoch
    size = len(trainloader.dataset)
    correct_train, total_train, epoch_loss_train = 0, 0, 0.0
    for batch in trainloader:
            x, labels = batch[0], batch[1]
            optimizer.zero_grad()
            zero_prob, mu, dispersion = net(x)
            loss = zimnb_loss(zero_prob, mu, dispersion, labels)
            loss.backward()
            optimizer.step()
            epoch_loss_train += loss.item()
            total_train += labels.size(0)
            predicted_labels = (zero_prob > 0.5).float()
            all_predictions.extend(predicted_labels.tolist())
            all_labels.extend(labels.tolist())
            # Compute accuracy for each sample and each label
            correct_train += ((predicted_labels == labels).float().sum(dim=1) == labels.size(1)).sum().item()

    loss_history.append(epoch_loss_train)
    epoch_acc = correct_train / total_train
    epoch_loss_train /= len(trainloader)
    macro_f1 = f1_score(all_labels, all_predictions, average='macro')
    micro_f1 = f1_score(all_labels, all_predictions, average='micro')
    #print(f"Epoch {epoch+1}: train loss {epoch_loss_train}, accuracy {epoch_acc}, macro F1 {macro_f1}, micro F1 {micro_f1}")

    return loss_history, epoch_acc


def val(net, valloader):
    """Evaluate the network on the entire test set."""
    all_predictions, all_labels, macro, micro = [], [], [], []
    loss_history, epoch_loss = [], []
    # output_dim = 8 # Uncomment for dynamic weighted BCE loss
    criterion = torch.nn.BCELoss()
    #criterion = DiceLoss(smooth=1e-6)
    #criterion = DiceMacroLoss(smooth=1e-6)
    correct, total, loss = 0, 0, 0.0
    net.eval()
    with torch.no_grad():
        for batch in valloader:
            features, labels = batch[0], batch[1]
            zero_prob, mu, dispersion = net(features)
            loss += zimnb_loss(zero_prob, mu, dispersion, labels).item()
            #_, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            predicted_labels = (zero_prob > 0.5).float()
            all_predictions.extend(predicted_labels.tolist())
            all_labels.extend(labels.tolist())
            # Compute accuracy for each sample and each label
            correct += ((predicted_labels == labels).float().sum(dim=1) == labels.size(1)).sum().item()
    loss /= len(valloader.dataset)
    loss_history.append(loss)
    accuracy = correct / total
    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='macro')
    macro_f1 = f1_score(all_labels, all_predictions, average='macro')
    micro_f1 = f1_score(all_labels, all_predictions, average='micro')
    #print(f"Epoch {epoch+1}: Val loss {loss}, accuracy {accuracy}, macro F1 {macro_f1}, micro F1 {micro_f1}")
    macro.append(macro_f1)
    micro.append(micro_f1)
    return loss_history, accuracy

def test(net, testloader):
    """Evaluate the network on the entire test set."""
    all_predictions, all_labels, macro, micro = [], [], [], []
    criterion = torch.nn.BCELoss()
    #output_dim = 8 # Uncomment for dynamic weighted BCE loss
    #criterion = DiceLoss(smooth=1e-6)
    #criterion = DiceMacroLoss(smooth=1e-6)
    correct, total, loss = 0, 0, 0.0
    net.eval()
    with torch.no_grad():
        for batch in testloader:
            features, labels = batch[0], batch[1]
            zero_prob, mu, dispersion = net(features)
            loss += zimnb_loss(zero_prob, mu, dispersion, labels).item()
            #_, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            predicted_labels = (zero_prob > 0.5).float()
            all_predictions.extend(predicted_labels.tolist())
            all_labels.extend(labels.tolist())
            # Compute accuracy for each sample and each label
            correct += ((predicted_labels == labels).float().sum(dim=1) == labels.size(1)).sum().item()
    loss /= len(testloader.dataset)
    accuracy = correct / total
    precision, recall, f1, _ = precision_recall_fscore_support(all_labels, all_predictions, average='macro')
    macro_f1 = f1_score(all_labels, all_predictions, average='macro')
    micro_f1 = f1_score(all_labels, all_predictions, average='micro')
    average_precision = average_precision_score(all_labels, all_predictions)
    print(f" Test loss {loss}, Test accuracy {accuracy}, macro F1 {macro_f1}, micro F1 {micro_f1}")
    print(classification_report(all_labels, all_predictions))
    macro.append(macro_f1)
    micro.append(micro_f1)
    return loss, accuracy

model = Net()
# Training loop
num_epochs = 100
train_losses = []
val_losses = []
epoches = []
for epoch in range(num_epochs):
    train_loss, train_acc = train(model, trainloaders, epoch)
    val_loss, val_acc = val(model, valloaders)
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    epoches.append(epoch+1)
    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')

# Evaluation on test set
test_loss, test_acc = test(model, testloaders)

print(f'Test loss: {test_loss}, Test accuracy: {test_acc}')

# Plot loss over epochs
plt.plot(epoches, val_losses, label='Training Loss')
#plt.plot(epoches, val_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Training Loss Over Epochs')
plt.legend()
plt.show()

# # Plot both macro and micro F1 scores
# plt.plot(epoches, ma, label='Macro F1 Score')
# plt.plot(epoches, mi, label='Micro F1 Score')

# # Add labels and title
# plt.xlabel('Epochs')
# plt.ylabel('F1 Score')
# plt.title('Macro and Micro F1 Scores Over Epochs')

# # Add legend
# plt.legend()

# # Show plot
# plt.show()